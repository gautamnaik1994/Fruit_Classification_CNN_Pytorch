{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6915c578",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6018055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c756e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_with_pad(img, target_size=(100, 100), pad_color=(0, 0, 0)):\n",
    "    h, w = img.shape[:2]\n",
    "    target_w, target_h = target_size\n",
    "\n",
    "    scale = min(target_w / w, target_h / h)\n",
    "    new_w = int(w * scale)\n",
    "    new_h = int(h * scale)\n",
    "    resized_img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    delta_w = target_w - new_w\n",
    "    delta_h = target_h - new_h\n",
    "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "    padded_img = cv2.copyMakeBorder(resized_img, top, bottom, left, right,\n",
    "                                    borderType=cv2.BORDER_CONSTANT, value=pad_color)\n",
    "    return padded_img\n",
    "\n",
    "\n",
    "def split_data(data, val_ratio=0.2):\n",
    "    random.shuffle(data)\n",
    "    val_size = int(len(data) * val_ratio)\n",
    "    return data[val_size:], data[:val_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ed30d",
   "metadata": {},
   "source": [
    "**Train, Test and Validation Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e259b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path_train=\"../data/raw/ninjacart_data/train\"\n",
    "raw_path_test=\"../data/raw/ninjacart_data/test\"\n",
    "\n",
    "train_onions = glob.glob(raw_path_train + \"/onion/*.*\")\n",
    "train_potatoes = glob.glob(raw_path_train + \"/potato/*.*\")\n",
    "train_tomatoes = glob.glob(raw_path_train + \"/tomato/*.*\")\n",
    "train_indian_market=glob.glob(raw_path_train + \"/indian_market/*.*\")\n",
    "\n",
    "train_onions, val_onions = split_data(train_onions)\n",
    "train_potatoes, val_potatoes = split_data(train_potatoes)\n",
    "train_tomatoes, val_tomatoes = split_data(train_tomatoes)\n",
    "train_indian_market, val_indian_market = split_data(train_indian_market)\n",
    "\n",
    "test_onions = glob.glob(raw_path_test + \"/onion/*.*\")\n",
    "test_potatoes = glob.glob(raw_path_test + \"/potato/*.*\")\n",
    "test_tomatoes = glob.glob(raw_path_test + \"/tomato/*.*\")\n",
    "test_indian_market=glob.glob(raw_path_test + \"/indian_market/*.*\")\n",
    "\n",
    "all_train_data = train_onions + train_potatoes + train_tomatoes + train_indian_market\n",
    "all_test_data = test_onions + test_potatoes + test_tomatoes + test_indian_market\n",
    "all_val_data = val_onions + val_potatoes + val_tomatoes + val_indian_market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d669e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onions:  680 169 83\n",
      "Potatoes:  719 179 81\n",
      "Tomatoes:  632 157 106\n",
      "Indian Markets:  480 119 81\n"
     ]
    }
   ],
   "source": [
    "print(\"Onions: \", len(train_onions), len(val_onions), len(test_onions))\n",
    "print(\"Potatoes: \", len(train_potatoes), len(val_potatoes), len(test_potatoes))\n",
    "print(\"Tomatoes: \", len(train_tomatoes), len(val_tomatoes), len(test_tomatoes))\n",
    "print(\"Indian Markets: \", len(train_indian_market), len(val_indian_market), len(test_indian_market))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3542ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pth in all_test_data + all_train_data:\n",
    "    img = cv2.imread(pth)\n",
    "    output_path = pth.replace(\"raw\", \"processed\")\n",
    "    processed_img = resize_with_pad(img)\n",
    "    cv2.imwrite(output_path, processed_img)\n",
    "\n",
    "\n",
    "for pth in all_val_data:\n",
    "    img = cv2.imread(pth)\n",
    "    output_path = pth.replace(\"../data/raw/ninjacart_data/train\", \"../data/processed/ninjacart_data/val\")\n",
    "    processed_img = resize_with_pad(img)\n",
    "    cv2.imwrite(output_path, processed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a0e5241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.4136, 0.3702, 0.3049])\n",
      "Std: tensor([0.2880, 0.2694, 0.2525])\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/processed/ninjacart_data/train\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "mean = torch.zeros(3)\n",
    "std = torch.zeros(3)\n",
    "total_images_count = 0\n",
    "\n",
    "for images, _ in loader:\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "    total_images_count += batch_samples\n",
    "\n",
    "mean /= total_images_count\n",
    "std /= total_images_count\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f8d4fd",
   "metadata": {},
   "source": [
    "**Image Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e755b39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/processed/ninjacart_data\"\n",
    "\n",
    "def generate_dataloader(mean, std):\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # simulate camera flipping\n",
    "        transforms.RandomRotation(15),           # allow small camera tilt\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # natural lighting changes\n",
    "        transforms.RandomAffine(translate=(0.1, 0.1), degrees=0),  # small object shifts\n",
    "        \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)  \n",
    "    ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "\n",
    "    train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
    "    test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
    "    val_data = datasets.ImageFolder(data_dir + '/val', transform=val_transforms)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=32)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=32)\n",
    "\n",
    "    return {\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader,\n",
    "        'test_loader': test_loader,\n",
    "        'train_data': train_data,\n",
    "        'val_data': val_data,\n",
    "        'test_data': test_data\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7553cce8",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649106cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[0.4136, 0.3702, 0.3049]\n",
    "std=[0.2880, 0.2694, 0.2525]\n",
    "\n",
    "content = generate_dataloader(mean, std)\n",
    "train_loader = content['train_loader']\n",
    "val_loader = content['val_loader']\n",
    "test_loader = content['test_loader']\n",
    "train_data = content['train_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ff26d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['indian_market', 'onion', 'potato', 'tomato'],\n",
       " {'indian_market': 0, 'onion': 1, 'potato': 2, 'tomato': 3})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.classes, train_data.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55c669ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_loader, '../data/processed/dataset/train_loader.pth')\n",
    "torch.save(test_loader, '../data/processed/dataset/test_loader.pth')\n",
    "torch.save(val_loader, '../data/processed/dataset/val_loader.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc1e55e",
   "metadata": {},
   "source": [
    "## Resnet Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "634ce2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "content = generate_dataloader(mean, std)\n",
    "train_loader = content['train_loader']\n",
    "val_loader = content['val_loader']\n",
    "test_loader = content['test_loader']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03817357",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_loader, '../data/processed/dataset/train_loader_resnet.pth')\n",
    "torch.save(test_loader, '../data/processed/dataset/test_loader_resnet.pth')\n",
    "torch.save(val_loader, '../data/processed/dataset/val_loader_resnet.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
